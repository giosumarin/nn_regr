@misc{ Nobody06,
       author = "Nobody Jr",
       title = "My Article",
       year = "2006" }
       
@article{McCulloch:1943aa,
  added-at = {2016-11-26T13:19:29.000+0100},
  author = {McCulloch, Warren and Pitts, Walter},
  biburl = {https://www.bibsonomy.org/bibtex/2de679aa52afcdd125180792058807f71/machinelearning},
  date-added = {2008-11-14 13:53:15 -0800},
  date-modified = {2008-11-14 13:53:58 -0800},
  interhash = {3e8e0d06f376f3eb95af89d5a2f15957},
  intrahash = {de679aa52afcdd125180792058807f71},
  journal = {Bulletin of Mathematical Biophysics},
  keywords = {imported ml},
  pages = {115-133},
  timestamp = {2016-11-26T13:20:49.000+0100},
  title = {A Logical Calculus of Ideas Immanent in Nervous Activity},
  volume = 5,
  year = 1943
}

@inproceedings{Rumelhart20081B,
  title={1 Backpropagation : The Basic Theory},
  author={David E. Rumelhart and Richard Durbin and Richard Golden and Yves Chauvin},
  year={2008}
}

@article{journals/nn/Qian99,
  added-at = {2018-11-14T00:00:00.000+0100},
  author = {Qian, Ning},
  biburl = {https://www.bibsonomy.org/bibtex/25467c3fc1e5a8200fc01310208258c53/dblp},
  ee = {https://www.wikidata.org/entity/Q52019658},
  interhash = {2b93b2cc86fc9b2dc20e2e367344acb4},
  intrahash = {5467c3fc1e5a8200fc01310208258c53},
  journal = {Neural Networks},
  keywords = {dblp},
  number = 1,
  pages = {145-151},
  timestamp = {2018-11-15T14:32:05.000+0100},
  title = {On the momentum term in gradient descent learning algorithms.},
  url = {http://dblp.uni-trier.de/db/journals/nn/nn12.html#Qian99},
  volume = 12,
  year = 1999
}

@online{pict_neur,
 author  = "Wikiedia Commons,Egm4313.s12 (Prof. Loc Vu-Quoc)",
 urlseen = "22-03-20",
 url     = "https://commons.wikimedia.org/w/index.php?curid=72816083",
}

@online{pict_mnist,
 author  = "Wikiedia Commons,Josef Steppan",
 urlseen = "26-03-20",
 url     = "https://commons.wikimedia.org/w/index.php?curid=64810040",
}

@online{pict_kfold,
 author  = "Wikiedia Commons,Fabian Flöck",
 urlseen = "26-03-20",
 url     = "https://commons.wikimedia.org/wiki/File:K-fold_cross_validation.jpg",
}

@article{10.5555/2627435.2670313,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {neural networks, deep learning, regularization, model combination}
}
  
@misc{he2015delving,
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art
neural networks. In this work, we study rectifier neural networks for image
classification from two aspects. First, we propose a Parametric Rectified
Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU
improves model fitting with nearly zero extra computational cost and little
overfitting risk. Second, we derive a robust initialization method that
particularly considers the rectifier nonlinearities. This method enables us to
train extremely deep rectified models directly from scratch and to investigate
deeper or wider network architectures. Based on our PReLU networks
(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012
classification dataset. This is a 26% relative improvement over the ILSVRC 2014
winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass
human-level performance (5.1%, Russakovsky et al.) on this visual recognition
challenge.},
  added-at = {2017-12-09T13:36:49.000+0100},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  biburl = {https://www.bibsonomy.org/bibtex/2dd7f625aa57c0d1b9f91f8be8135a513/spdrnl},
  description = {[1502.01852] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  interhash = {0b13164e63812ff4bafd89b6139fe961},
  intrahash = {dd7f625aa57c0d1b9f91f8be8135a513},
  keywords = {deep prelu relu tech},
  note = {cite arxiv:1502.01852},
  timestamp = {2017-12-09T13:36:49.000+0100},
  title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
  ImageNet Classification},
  url = {http://arxiv.org/abs/1502.01852},
  year = 2015
}

@Inbook{Refaeilzadeh2009,
author="Refaeilzadeh, Payam
and Tang, Lei
and Liu, Huan",
editor="LIU, LING
and {\"O}ZSU, M. TAMER",
title="Cross-Validation",
bookTitle="Encyclopedia of Database Systems",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="532--538",
isbn="978-0-387-39940-9",
doi="10.1007/978-0-387-39940-9_565",
url="https://doi.org/10.1007/978-0-387-39940-9_565"
}

@article{cybenko,
  abstract = {{In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}},
  added-at = {2012-03-02T03:39:18.000+0100},
  author = {Cybenko, G.},
  biburl = {https://www.bibsonomy.org/bibtex/2be85c56ae384216b2e35bdf79b7fb477/baby9992006},
  citeulike-article-id = {3561150},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF02551274},
  citeulike-linkout-1 = {http://www.springerlink.com/content/n873j15736072427},
  day = 1,
  doi = {10.1007/BF02551274},
  interhash = {96aecb02daa11041489259a8edb54070},
  intrahash = {be85c56ae384216b2e35bdf79b7fb477},
  issn = {0932-4194},
  journal = {Mathematics of Control, Signals, and Systems (MCSS)},
  keywords = {approximation, control, duckling, free, lunch, no, theorem, theory, ugly, universal},
  month = dec,
  number = 4,
  pages = {303--314},
  posted-at = {2012-02-28 13:17:08},
  priority = {2},
  publisher = {Springer London},
  timestamp = {2012-03-02T03:39:20.000+0100},
  title = {{Approximation by superpositions of a sigmoidal function}},
  url = {http://dx.doi.org/10.1007/BF02551274},
  volume = 2,
  year = 1989
}

@article{han2015compression,
  abstract = {Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems with limited hardware
resources. To address this limitation, we introduce "deep compression", a three
stage pipeline: pruning, trained quantization and Huffman coding, that work
together to reduce the storage requirement of neural networks by 35x to 49x
without affecting their accuracy. Our method first prunes the network by
learning only the important connections. Next, we quantize the weights to
enforce weight sharing, finally, we apply Huffman coding. After the first two
steps we retrain the network to fine tune the remaining connections and the
quantized centroids. Pruning, reduces the number of connections by 9x to 13x;
Quantization then reduces the number of bits that represent each connection
from 32 to 5. On the ImageNet dataset, our method reduced the storage required
by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method
reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of
accuracy. This allows fitting the model into on-chip SRAM cache rather than
off-chip DRAM memory. Our compression method also facilitates the use of
complex neural networks in mobile applications where application size and
download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,
compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy
efficiency.},
  added-at = {2017-06-03T00:10:05.000+0200},
  author = {Han, Song and Mao, Huizi and Dally, William J.},
  biburl = {https://www.bibsonomy.org/bibtex/20a862efa8c33bda34d56dbe23a07a8e1/hprop},
  description = {[1510.00149] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  interhash = {2ba770eb4cda1bfa599aecfc6fdd2914},
  intrahash = {0a862efa8c33bda34d56dbe23a07a8e1},
  keywords = {compression distillation machine-learning},
  note = {cite arxiv:1510.00149Comment: Published as a conference paper at ICLR 2016 (oral)},
  timestamp = {2017-06-03T00:10:05.000+0200},
  title = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained
  Quantization and Huffman Coding},
  url = {http://arxiv.org/abs/1510.00149},
  year = 2015
}

@article{L2,
	title={L2 Regularization versus Batch and Weight Normalization},
	author={Twan van Laarhoven},
	journal={ArXiv},
	year={2017},
	volume={abs/1706.05350}}

