\documentclass[11pt,a4paper,twoside,
openright]{book}
\usepackage[italian]{babel}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{tikz-cd}
\usepackage{cite}
\usepackage{chngcntr}
\usepackage[utf8x]{inputenc}
\usepackage{enumerate}
\usepackage{makeidx}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{url}

\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{fadings}
\usetikzlibrary{positioning,calc}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}
\tikzset{basic/.style={draw,fill=blue!50!green!20,
                       text badly centered,minimum width=3em}}
\tikzset{input/.style={basic,circle}}
\tikzset{weights/.style={basic,rectangle,minimum width=2em}}
\tikzset{functions/.style={basic,circle,fill=blue!50!green!20}}
\newcommand{\addsymbol}{\draw[thick] (0.5em,0.5em) -- (0,0.5em) -- 
                        (0,-0.5em) --  (-0.5em,-0.5em)
                        (0em,0.75em) -- (0em,-0.75em)
                        (0.75em,0em) -- (-0.75em,0em);}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\renewcommand{\contentsname}{Indice}
\newcommand{\matr}[1]{\mathbf{#1}} 

\begin{document}

\begin{titlepage}
	
	\begin{figure}
		\centering
		\includegraphics[width=424pt]{tesiSCIENZE_TECNOLOGIE.jpg}%A QUESTO LINK TROVATE I MARCHI PER LA TESI AGGIORNATI E DIVISI PER FACOLTà: http://www.unimi.it/ateneo/37094.htm
		\vspace{0.5 cm}
	\end{figure}
	
%DATO CHE NEI MARCHI SONO GIà PRESENTI SIA IL NOME DELL'UNIVERSITà SIA QUELLO DELLA FACOLTà, NON VANNO RISCRITTI. IN OGNI CASO AGGIUNGO IN COMMENTO COME SAREBBERO:	
%\begin{center}
%	{\Huge \textsc{Università degli Studi di Milano} }\\
%\end{center}
%\begin{center}
%{\Huge Facoltà XXXXX}\\
%\end{center}

\begin{center}
{\LARGE Corso di Laurea in Informatica}
\end{center}

\begin{center}
\vspace{3 cm}
{\Large \textsc{Compressione di reti neurali \\ in problemi  di classificazione e regressione} }
\end{center}
\par
  \vspace{3 cm}
  
  \begin{flushleft}
  		 Relatore:\\Prof. Dario MALCHIODI\\
		 
  		 \noindent Correlatore:\\Dr. Marco FRASCA
  \end{flushleft}
  \vspace{1 cm}
  \begin{flushright}
  	Tesi di Laurea di:\\ Giosuè Cataldo Marinò\\ Matricola: 829404
  \end{flushright}
    	  
\vfill
\begin{center}
	{\large Anno Accademico 2018/2019}
\end{center}

\end{titlepage}


\tableofcontents

\chapter*{Introduzione}
BLABLA
Blablabla said Nobody ~\cite{Nobody06}.
\chapter{Reti Neurali}
\section{Reti Neurali Biologiche}
I neuroni sono delle celle elettricamente attive e il sistema nervoso centrale ne contiene circa $10^{11}$. La maggior parte di essi ha la forma indicata in Figura~\ref{fig:neurbio}. I dendriti rappresentano gli ingressi del neurone mentre l’assone ne rappresenta l’uscita. La comunicazione tra i neuroni avviene alle giunzioni, chiamate sinapsi. Ogni neurone è tipicamente connesso ad un migliaio di altri neuroni e, di conseguenza, il numero di sinapsi nel cervello supera $10^{14}$.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=200pt]{BioNeuron.png}
\caption{Neurone Biologico~\cite{pict_neur}}
\end{center}
\label{fig:neurbio}
\end{figure}

Ogni neurone si può trovare principalmente in 2 stati: attivo o a riposo. Quando il neurone si attiva, esso produce un potenziale di azione (impulso elettrico) che viene trasportato lungo l’assone. Una volta che il segnale raggiunge la sinapsi esso provoca il rilascio di sostanze chimiche (neurotrasmettitori) che attraversano la giunzione ed entrano nel corpo di altri neuroni. In base al tipo di sinapsi, che possono essere eccitatori o inibitori, queste sostanze aumentano o diminuiscono rispettivamente la probabilità che il successivo neurone si attivi. Ad ogni sinapsi è associato un peso che ne determina il tipo e l’ampiezza dell’effetto eccitatore o inibitore. Quindi, in poche parole, ogni neurone effettua una somma pesata degli ingressi provenienti dagli altri neuroni e, se questa somma supera una certa soglia, il neurone si attiva.\\
Ogni neurone, operando ad un ordine temporale del millisecondo, rappresenta un sistema di elaborazione relativamente lento; tuttavia, l’intera rete ha un numero molto elevato di neuroni e sinapsi che possono operare in modo parallelo e simultaneo, rendendo l’effettiva potenza di elaborazione molto elevata. Inoltre la rete neurale biologica ha un’alta tolleranza ad informazioni poco precise (o sbagliate), ha la facoltà di apprendimento e generalizzazione.

\section{Reti Neurali Artificiali}
\label{section:retiartificiali}
Ci concentreremo su una classe particolare di modelli di reti neurali: le reti a catena aperta. Queste reti possono essere viste come funzioni matematiche non lineari che trasformano un insieme di variabili indipendenti $x = (x_{1}, ... , x_{d})$, chiamate ingressi della rete, in un insieme di variabili dipendenti $y = (y_{1}, ... , y_{c})$, chiamate uscite della rete. La precisa forma di queste funzioni dipende dalla struttura interna della rete e da un insieme di valori $w = (w_{1}, ... , w_{d})$, chiamati pesi. Possiamo quindi scrivere la funzione della rete nella forma $y = y(x; w)$ che denota il fatto che $y$ sia una funzione di $x$ parametrizzata da $w$.

\begin{figure}[h!]
\begin{center}
\tikzset{basic/.style={draw,fill=white,text width=1em,text badly centered}}
\tikzset{input/.style={basic,circle}}
\tikzset{weights/.style={basic,rectangle}}
\tikzset{functions/.style={basic,circle,fill=white}}
\begin{tikzpicture}[scale=1.2]
    \foreach \h [count=\hi ] in {$x_d$,$x_2$,$x_1$,$1$}{%
          \node[input] (f\hi) at (0,\hi*2cm-5 cm) {\h};
        }
    \node[input] (z) at (10,0) {$z$};
    \node[functions] (sum) at (4,0) {$\sum$};
    \foreach \h [count=\hi ] in {$w_d$,$w_2$,$w_1$,$b$}{%
          \path (f\hi) -- node[weights] (w\hi) {\h} (sum);
          \draw[->] (f\hi) -- (w\hi);
          \draw[->] (w\hi) -- (sum);
        }        
    \node[functions] (step) at (7,0) {};
       \begin{scope}[xshift=7cm,scale=.75]
         \addsymbol
       \end{scope}
    \draw[->] (sum) -- (step);
    \draw[->] (step) -- (z);%++(1,0);
    % Labels
    \node[above=.5cm]  at (f4) {Inputs};
    \node[above=.5cm] at (w4) {Weights};
    
\end{tikzpicture}
\end{center}
\caption{Modello di McCulloch-Pitts}
\label{fig:mcpitts}
\end{figure}
\subsection*{Modello di McCulloch-Pitts}

Un semplice modello matematico di un singolo neurone è quello rappresentato in Figura~\ref{fig:mcpitts} ed è stato proposto da McCulloch e Pitts~\cite{McCulloch:1943aa} alle origini delle reti neurali. Esso può essere visto come una funzione non lineare che trasforma le variabili di ingresso $x_{1}, ..., x_{d}$ nella variabile di uscita $z$. Nell’elaborato ci riferiremo a questo modello come unità di elaborazione, o semplicemente unità.
In questo modello, viene effettuata la somma ponderata degli ingressi, usando come pesi i valori $w_{1}, ..., w_{d}$ (che sono analoghi alle potenze delle sinapsi nella rete biologica), ottenendo così
\begin{equation}
a = \sum\limits_{i=1}^d w_{i}x_{i}+b
\label{som+b}
\end{equation}
dove il parametro $b$ viene chiamato bias (corrisponde alla soglia di attivazione del neurone biologico). Se definiamo un ulteriore ingresso $x_{0}$, impostato costantemente a 1, possiamo scrivere~\eqref{som+b} come
\begin{equation}
a = \sum\limits_{i=0}^d w_{i}x_{i}
\label{som}
\end{equation}
dove $x_{0}$ = 1. Precisiamo che i valori dei pesi possono essere di qualsiasi segno, che dipende dal tipo di sinapsi. L’uscita $z$ (che può essere vista come tasso medio di attivazione del neurone biologico) viene ottenuta applicando ad $a$ una trasformazione non lineare $g$, chiamata funzione di attivazione, ottenendo 
\begin{equation}
z=g(a)=g\left( \sum\limits_{i=0}^d w_{i}x_{i} \right).
\label{act+som}
\end{equation}
Il modello originale di McCulloch-Pitts usava come attivazione la funzione gradino
\begin{equation}
g(a)=
\begin{cases}
1 &\text{ se } a\geq0, \\
-1 &\text{ altrimenti}.
\end{cases}
\label{act+som}
\end{equation}

\section{Addestramento della rete}
\label{addestramento}
Abbiamo detto che una rete neurale può essere rappresentata dal modello matematico $y = y(x; w)$, che è una funzione di $x$ parametrizzata dai pesi $w$. Prima di poter utilizzare questa rete, dobbiamo identificare il modello, ovvero dobbiamo determinare tutti i parametri $w$. Il processo di determinazione di questi parametri è chiamato addestramento e può essere un’azione molto intensa dal punto di vista computazionale. Tuttavia, una volta che sono stati definiti i pesi, nuovi ingressi possono essere elaborati molto rapidamente.
Per addestrare una rete abbiamo bisogno di un insieme di esempi, chiamato insieme di addestramento (\textit{training set}), i cui elementi sono coppie $(x^{q}, t^{q})$, $q = 1, ..., n$, dove $t^{q}$ rappresenta il valore di uscita desiderato, chiamato target, in corrispondenza dell'ingresso $x^{q}$. 
L’addestramento consiste nella ricerca dei valori per i parametri $w$ che minimizzano un’opportuna funzione di errore. Ci sono diverse forme di questa funzione, la più usata risulta essere la somma dei quadrati residui. I residui sono definiti come
\begin{equation}
r_{qk} = y_{k}\left(x^{q}; w\right) - t^{q}_{k}
\label{res}
\end{equation}
dove $k$ rapppresenta l'indice dei neuroni di output.
La funzione di errore $E$ risulta allora essere
\begin{equation}
E = \sum\limits_{q=1}^n \sum\limits_{k=1}^c r_{qk}^{2}. 
\label{quadres}
\end{equation}
\`E facile osservare che $E$ dipende da $x^{q}$ e da $t^{q}$ che sono valori noti e da $w$ che è incognito, quindi $E$ è in realtà una funzione dei soli pesi $w$.

\section{Funzioni di attivazione}
Come già introdotto nel Paragrafo~\ref{section:retiartificiali}, le funzioni di attivazione determinano l'output di una rete neurale.
Le funzioni utilizzate principalmente negli esperimenti di questo elaborato sono tre: \textit{sigmoid}, \textit{ReLU} (\textit{Rectified Linear Units}) e \textit{LeakyReLU} descritte in (\ref{sigmoid} -\ref{lrelu}).
Le funzioni di attivazione sono non-lineari e la loro derivata è calcolabile in modo analitico per velocizzare la computazione.

\begin{equation}
\mathrm{sigmoid}(a) = \frac{1}{1+e^{-a}}, \qquad
\mathrm{sigmoid}'(a) = \mathrm{sigmoid}(a)(1-\mathrm{sigmoid}(a)).
\label{sigmoid}
\end{equation}

\begin{equation}
\mathrm{ReLU}(a) = \max(0,a), \qquad
ReLU'(a) = \begin{cases}
1 &\text{ se } a\geq0, \\
0 &\text{ altrimenti}.
\end{cases}
\label{relu}
\end{equation}

\begin{equation}
\mathrm{LeakyReLU}(a) = \begin{cases}
a &\text{ se } a\geq0, \\
- \alpha a &\text{ altrimenti}.
\end{cases} \qquad
\mathrm{LeakyReLU}'(a) = \begin{cases}
1 &\text{ se } a\geq0, \\
-\alpha &\text{ altrimenti}.
\end{cases}
\label{lrelu}
\end{equation}
dove $\alpha$ è un parametro numerico.

La scelta della funzione è guidata dal tipo di problema che si vuole affrontare, per esempio se vogliamo un output compreso tra 0 e 1 sarà più adeguato utilizzare una funzione \textit{sigmoid} rispetto ad una \textit{ReLU}.



\section{MultiLayer Perceptron}
In questo paragrafo parleremo in dettaglio del modello mulilayer perceptron introducendo il concetto di multistrato e descriveremo il metodo di addestramento utilizzato principalmente nel Capitolo~\ref{esperimenti}. Nell’elaborato ci riferiremo a questo modello come MLP.
\subsection{Architettura del modello MLP}
\paragraph{Modello a uno strato}
\def\layersep{2.5cm}

\begin{figure}
\begin{center}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[basic,circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=white];
    \tikzstyle{output neuron}=[neuron, fill=white];
    \tikzstyle{hidden neuron}=[neuron, fill=white];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {0,...,3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron] (I-\name) at (0,-\y-1) {$x_\y$};
	\node[input neuron] (I-d) at (0,-4-1) {$x_d$};
    \node (dot) at (0,-4.4) {$\vdots$};
    %\node[annot,pin={[pin edge={->}]right:bias}, left of=I-0] (O) {};
	\node[annot,pin={[pin edge={->}]below:},above of=I-0, node distance=1.5cm] {$bias$};
    % Draw the output layer node
    \node[output neuron, right of=I-1] (O) {$z_1$};
    \node[right of=I-2] (dot) {$\vdots$};
    \node[output neuron, right of=I-3] (O2) {$z_m$};
	%\node[below of=I-1] (dots) {$\vdots$} -- (dots) node[left of=dots] (ldots) {$\vdots$};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {0,...,3}
    	\path (I-\source) edge (O);
    \foreach \source in {0,...,3}
    	\path (I-\source) edge (O2);
    \path (I-d) edge (O);
    \path (I-d) edge (O2);

    % Annotate the layers
    
    %\node[annot, node distance=1cm] (input) {Input layer};
    \node[annot, below of=I-d, node distance=2cm] (input) {Input layer};
    \node[annot, right of=input] {Output layer};
\end{tikzpicture}
\end{center}
\caption{MLP a uno strato}
\label{fig:MLP1}
\end{figure}
Nel paragrafo precedente abbiamo trattato la singola unità di elaborazione descritta in~\eqref{act+som}. Se consideriamo ora un insieme di $m$ unità, con ingressi comuni, otteniamo una rete neurale a singolo strato come in Figura~\ref{fig:MLP1}. Le uscite di questa rete sono date da
\begin{equation}
z_j = g\left(\sum\limits_{i=0}^d w_{ij}x_i\right), \quad
j=1,...,m
\label{rete1}
\end{equation}
dove $w_{ij}$ rappresenta il peso che connette l'ingresso $i$ con l'uscita $j$; $g$ è la funzione di attivazione e $x_0=1$ per incorporare il bias, come precedentemente spiegato.

\paragraph{Modello a due strati}
\begin{figure}
\begin{center}
\def\layersep{1.5cm}

\begin{tikzpicture}[
   shorten >=1pt,->,
   draw=black!50,
    node distance=\layersep,
    every pin edge/.style={<-,shorten <=1pt},
    neuron/.style={circle,basic,fill=black!25,minimum size=17pt,inner sep=0pt},
    input neuron/.style={neuron, fill=white},
    output neuron/.style={neuron, fill=white},
    hidden neuron/.style={neuron, fill=white},
    annot/.style={text width=4em, text centered}
]
	
	\foreach \name / \y in {0,...,2}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron] (I-\name) at (0,-\y-1) {$x_\y$};
	\node[input neuron] (I-d) at (0,-3-1) {$x_d$};
    \node (dot) at (0,-3.4) {$\vdots$};

    % Draw the input layer nodes
    %\foreach \name / \y in {1,...,4}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
    %    \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-\y) {};

    % set number of hidden layers
    \newcommand\Nhidden{1}

    % Draw the hidden layer nodes
    
	\foreach \y in {0,...,4}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron] (H-\y) at (\layersep,-\y) {$z_\y$};
	\node[input neuron] (H-m) at (\layersep,-5) {$z_m$};    
    \node (dot) at (\layersep,-4.4) {$\vdots$};

    %\foreach \N in {1,...,\Nhidden} {
    %   \foreach \y in {1,...,5} {
    %      \path[yshift=0.5cm]
    %          node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y cm) {};
    %       }
    \node[annot,below of=H-m, node distance=2cm] (hl) {Hidden layer};
    
	\node[annot,pin={[pin edge={->}]below:},above of=I-0] {$bias$};
	\node[annot,pin={[pin edge={->}]below:},above of=H-0] {$bias$};
    % Draw the output layer node
    \node[input neuron, right of=H-2] (O) {$y_1$};
    \node[input neuron, below of=O] (Oc) {$y_c$};
    %\node (dot) at (\layersep*2,-2.6) {$\vdots$};
	\node(dot) at ($(O)!0.4!(Oc)$) {$\vdots$};
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {0,...,2}
        \foreach \dest in {0,...,4}
            \path (I-\source) edge (H-\dest);
	\foreach \dest in {0,...,4}     
     	\path (I-d) edge (H-\dest);  
    \foreach \source in {0,1,2,d}
            \path (I-\source) edge (H-m);      

    % connect all hidden stuff
   % \foreach [remember=\N as \lastN (initially 1)] \N in {1}%{2,...,\Nhidden}
    %   \foreach \source in {1,...,5}
     %      \foreach \dest in {1,...,5}
      %         \path (H\lastN-\source) edge (H\N-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {0,...,4}
        \path (H-\source) edge (O);
	\path (H-m) edge (O);
    % Annotate the layers
    \foreach \source in {0,...,4}
        \path (H-\source) edge (Oc);
	\path (H-m) edge (Oc);

    \node[annot,left of=hl] {Input layer};
    \node[annot,right of=hl] {Output layer};
\end{tikzpicture}
\caption{MLP a due strati}
\label{fig:MLP2}
\end{center}
\end{figure}
Per ottenere reti più potenti\footnote{un percettrone mono-strato non è in grado di esprimere tutte le funzioni possibili, mentre un percettrone a più strati lo è~\cite{cybenko}.} è necessario considerare reti aventi più strati chiamate \textit{multilayer perceptron} come in Figura~\ref{fig:MLP2}. Le unità centrali rappresentano lo strato nascosto (\textit{hidden}) perchè il valore di attivazione delle singole unità di questo strato non sono misurabili dall'esterno. L'attivazione di queste unità è data da~\eqref{rete1}. Le uscite della rete vengono ottenute tramite una seconda trasformazione, analoga alla prima, sui valori $z_j$ ottenendo
\begin{equation}
y_k = \tilde{g}\left(\sum\limits_{j=0}^m \tilde{w}_{jk}z_i\right), \quad
k=1,\dots,c,
\label{rete2}
\end{equation}
dove $\tilde{w}_{jk}$ rappresenta il peso del secondo strato che connette l'unità nascosta $j$ all'unità di uscita $k$. Sostituendo~\eqref{rete1} in~\eqref{rete2} otteniamo
\begin{equation}
y_k = \tilde{g}\left( \sum\limits_{j=0}^m \tilde{w}_{jk}g\left(\sum\limits_{i=0}^d w_{ij}x_i\right) \right), \quad
k=1,\dots,c.
\label{rete2.1}
\end{equation}
La funzione di attivazione $\tilde{g}$, applicata alle unità di uscita, può essere diversa dalla funzione di attivazione $g$, applicata alle unità nascoste.

Per ottenere una capacità di rappresentazione universale, la funzione di attivazione $g$ delle unità nascoste deve essere non lineare~\cite{cybenko}. Se $g$ e $\tilde{g}$ fossero entrambe lineari, ~\eqref{rete2.1} diventerebbe un prodotto tra matrici, che è esso stesso una matrice. Inoltre, come vedremo più avanti, le funzioni di attivazione devono essere differenziabili.
\subsection{Addestramento}
L'addestramento consiste nella ricerca dei valori $\textbf{w}=(w_1,..,w_n)$ \footnote{con $\textbf{w}$ intendiamo i pesi di ogni strato} che minimizzano la funzione di errore $E(\textbf{w})$ (vista precedentemente nel Capitolo~\ref{addestramento}).
La ricerca del minimo avviene in modo iterativo partendo da un valore iniziale $\textbf{w}$, scelto in modo casuale o tramite un criterio. Alcuni algoritmi trovano il minimo locale più vicino al punto iniziale, mentre altri riescono a trovare il minimo globale.

Diversi algoritmi di ricerca del punto minimo fanno uso delle derivate parziali della funzione di errore $E$, ovvero del suo vettore gradiente $\nabla E$. Questo vettore indica la direzione ed il verso di massima crescita di $E$ nel punto $\textbf{w}$.
\subsection*{Error back-propagation}
L'algoritmo di \textit{Error back-propagation}~\cite{Rumelhart20081B} confronta il valore in uscita con il valore desiderato. Sulla base della differenza calcolata, l'algoritmo modifica i pesi della rete neurale, facendo convergere progressivamente il set dei valori di uscita verso quelli desiderati.
Consideriamo come funzione errore la somma dei quadrati residui~\eqref{quadres}.
\begin{equation}
E = \sum\limits_{q=1}^n E^{q}, \qquad
E^q = \sum\limits_{k=1}^c [y_k (x^q;w) - t_{k}^{q}]^2.
\label{resq}
\end{equation}

Possiamo vedere $E$ come somma di $E^q$ che corrisponde alla coppia $(x^q,t^q)$. Grazie alla linearità della derivazione possiamo calcolare la derivata di $E$ come somma delle derivate dei termini $E^q$. Nel seguito omettiamo l'indice $q$: i passaggi indicati si riferiscono ad un singolo caso $q$ ma le operazioni sono fatte per ogni valore di $q$. Consideriamo un esempio di rete neurale MLP con uno strato hidden.
\begin{equation}
y_k=\tilde{g}(\tilde{a}_k), \qquad
 a_k=\sum\limits_{j=0}^m \tilde{w}_{jk}z_j.
\label{ak}
\end{equation}
La derivata di $E^q$ rispetto ad un generico peso $w_{jk}$ dello strato hidden è
\begin{equation}
\frac{\partial E^q}{\partial \tilde{w}_{jk}}=\frac{\partial E^q}{\partial \tilde{a}_k}\frac{\partial \tilde{a}_k}{\partial w_{jk}}
\label{chain}
\end{equation}
e tramite~\eqref{ak} otteniamo
\begin{equation}
\frac{\partial \tilde{a}_k}{\partial \tilde{w}_{jk}}=z_j.
\label{chain1}
\end{equation}
Con~\eqref{ak} e~\eqref{resq} otteniamo
\begin{equation}
\frac{\partial E^q}{\partial \tilde{a}_k}=\tilde{g}'(\tilde{a}_k)[y_k-t_k],
\label{chain2}
\end{equation}
possiamo ora riscrivere~\eqref{chain} come
\begin{equation}
\frac{\partial E^q}{\partial w_{jk}}=\tilde{g}'(\tilde{a}_k)[y_k-t_k]z_j.
\label{chain2}
\end{equation}
Definiamo
\begin{equation}
\tilde{\delta}_k = \frac{\partial E^q}{\partial \tilde{a}_k}=\tilde{g}'(\tilde{a}_k)[y_k-t_k]
\label{delta}
\end{equation}
ottenendo una semplice espressione per la derivata di $E^q$ rispetto a $w_{jk}$
\begin{equation}
\frac{\partial E^q}{\partial \tilde{w}_{jk}}=\tilde{\delta}_k z_j.
\label{deltaz}
\end{equation}



Per quanto riguarda le derivate rispetto ai pesi del primo strato riscriviamo
\begin{equation}
z_j=g(a_j), \qquad
 a_j=\sum\limits_{i=0}^d w_{ij}x_i.
\label{aj}
\end{equation}
Possiamo quindi scrivere la derivata come
\begin{equation}
\frac{\partial E^q}{\partial w_{ij}}=\frac{\partial E^q}{\partial a_j}\frac{\partial a_j}{\partial w_{ij}}.
\label{chain3}
\end{equation}
In modo analogo, osservando~\eqref{aj}, otteniamo
\begin{equation}
\frac{\partial a_j}{\partial w_{ij}}=x_i.
\label{chain4}
\end{equation}
Per il calcolo della derivata di $E^q$ rispetto ad $a_j$, usando la \textit{chain-rule} abbiamo
\begin{equation}
\frac{\partial E^q}{\partial a_j}=\sum\limits_{k=1}^c \frac{\partial E^q}{\partial \tilde{a}_k}\frac{\partial \tilde{a}_k}{\partial a_j},
\label{chain5}
\end{equation}
dove la derivata di $E^q$ rispetto ad $\tilde{a}_k$ è data da~\eqref{chain2}, mentre la derivata di $\tilde{a}_k$ rispetto ad $a_j$ si trova usando~\eqref{ak} e~\eqref{aj}; quindi
\begin{equation}
\frac{\partial \tilde{a}_k}{\partial a_j}=\tilde{w}_{jk}g'(a_j).
\label{chain6}
\end{equation}
Usando~\eqref{delta},~\eqref{chain5} e~\eqref{chain6} otteniamo
\begin{equation}
\frac{\partial E^q}{\partial a_{j}}=g'(a_j)\sum\limits_{k=1}^c \tilde{w}_{jk}\tilde{\delta}_k.
\label{chain7}
\end{equation}
Possiamo quindi riscrivere~\eqref{chain3} come
\begin{equation}
\frac{\partial E^q}{\partial w_{ij}}=g'(a_j)x_i\sum\limits_{k=1}^c w_{jk}\delta_k
\label{chain8}
\end{equation}
e, come abbiamo fatto in~\eqref{delta}, poniamo
\begin{equation}
\delta_j = \frac{\partial E^q}{\partial a_j}=g'(a_j)\sum\limits_{k=1}^c \tilde{w}_{jk}\tilde{\delta}_k,
\label{delta2}
\end{equation}
ottenendo infine
\begin{equation}
\frac{\partial E^q}{\partial w_{ij}}=\delta_j x_i
\label{deltax}
\end{equation}
che ha la stessa semplice forma di~\eqref{deltaz}.
Elenchiamo quindi i passi da seguire per valutare la derivata della funzione $E$:
\begin{itemize}
\item Per ogni coppia $(x^q,t^q)$ valutare le attivazioni delle unità nascoste e di uscita usando le equazioni~\eqref{aj} e~\eqref{ak};
\item Valutare il valore $\tilde{\delta}_k$ per $k=1,..,c$ usando equazione~\eqref{delta};
\item Valutare il valore $\delta_j$ per $j=1,..,m$ usando equazione~\eqref{delta2};
\item Valutare il valore di $E^q$ usando le equazioni~\eqref{deltax} e~\eqref{deltaz};
\item Ripetere i passi precedenti per ogni coppia $(x^q,t^q)$ del \textit{training set} e sommare tutte le derivate per ottenere la derivata della funzione errore $E$.
\end{itemize}

Dopo il calcolo delle derivate i pesi di ogni strato verranno aggiornati come
\begin{equation}
w_{ij}^{(t)}=w_{ij}^{(t-1)} + \Delta w_{ij}^{(t)},
\label{update_w_ij}
\end{equation}
\begin{equation}
\Delta w_{ij}^{(t)} = -\eta \nabla E(w_{ij}^{(t)}),
\label{deltawij}
\end{equation}
dove $\eta>0$ è il coefficiente di apprendimento (\textit{learning rate}): più $\eta$ è grande più imparerà velocemente, valori troppo grandi di $\eta$ fanno divergere la rete.
Questo verrà ripetuto iterativamente ogni epoca, dove con epoca intendiamo la visione dell'intero \textit{training set}.
L'aggiornamento dei pesi durante ogni epoca può avvenire dopo ogni elemento (\textit{online}), dopo tutti gli elementi (\textit{batch}) o dopo un numero parametrico di esempi (\textit{mini-batch}).
Negli esperimenti di questo elaborato viene utilizzata la modalità \textit{mini-batch} perché permette al modello di convergere più velocemente.

Per migliorare la convergenza della rete abbiamo utilizzato la versione di discesa del gradiente con \textit{momento}~\cite{journals/nn/Qian99}.
In questa versione l'equazione~\eqref{deltawij} diventa:
\begin{equation}
w_{ij}^{(t)}=- \eta \nabla E(w_{ij}^{(t)}) + \mu \Delta w_{ij}^{(t-1)},
\label{momentum}
\end{equation}
dove $\mu$ è un parametro aggiuntivo nell'intervallo $[0,1)$ che valorizza quanto considerare il gradiente dell'epoca precedente. 
Questo tipo di aggiornamento accelererà la convergenza se il verso del gradiente è lo stesso dell'epoca precedente. 

\subsection*{Criteri di arresto}
\label{stopping}
la procedura di addestramento del paragrafo precedente viene iterata fino a che non risulta soddisfatto un prefissato criterio di arresto. Negli esperimenti descritti nel Capitolo ~\ref{esperimenti}, sono stati utilizzati diversi criteri, quali:

\begin{itemize}
\item Stop dopo un numero prefissato di epoche;
\item Stop dopo che l'errore/accuratezza non migliora rispetto all'epoca precedente;
\item Stop dopo che l'errore/accuratezza non migliora rispetto all'epoca precedente con \textit{patience}, ovvero attendendo in ogni caso un numero finito di epoche senza miglioramento delle prestazioni prima di interrompersi.
\end{itemize}

\chapter{Metodi di compressione}
In questo capitolo descriveremo nel dettaglio come funzionano gli algoritmi di compressione utilizzati nel Capitolo~\ref{esperimenti}.
\section{Pruning}
Il pruning consiste nel tagliare le connessioni da una rete addestrata per poi riaddestrarla senza le connessioni tagliate.
Oltre ad un vantaggio computazionale può portare a una generalizzazione che permette di ridurre l'overfitting (ovvero imparare troppo dagli esempi del training set).
\subsection{Strutture dati necessarie}
Dopo aver tagliato, disattivato le connessioni e riaddestrato, la matrice sarà più o meno sparsa (in base a quante connessioni tagliamo). Per ridurre lo spazio viene utilizzata una rappresentazione matriciale CSC (Compressed Sparse Column)\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html}}.
Questo tipo di matrice è una struttura basata sull'indicizzazione tramite colonne di una matrice sparsa. Viene descritta da tre vettori:
\begin{itemize}
\item il primo in cui vengono salvati i valori non nulli dal primo elemento in alto a destra proseguendo verso il basso e successivamente a destra;
\item il secondo corrisponde all'indice delle righe dei valori;
\item il terzo indica gli indici dei valori in cui ogni colonna inizia.
\end{itemize}

Questo tipo di struttura dati richiede il salvataggio di $2a+c+1$ dove $a$ è il numero di valori non zero e $c$ il numero di colonne.
\subsection{Tecniche implementative}
\label{percentile}
Durante la configurazione della rete viene aggiunta una procedura che esegue il pruning sulle matrici delle connessioni addestrate in precedenza. Identifichiamo con $\tau$ la soglia entro cui i pesi verranno eliminati, la nuova matrice sarà definita come:
\begin{equation}
w_{ij}=
\begin{cases}
0 \text{ se } |w_{ij}|<\tau, \\
w_{ij} \text{ altrimenti}.
\end{cases}
\label{pruning}
\end{equation}
Abbiamo scelto $\tau$ come il quantile $q$ della distribuzione del valore assoluto dei pesi, dove $q$ assume i valori in [0,1].

\subsection{Tasso di compressione}
\label{sectassopr}
Il tasso di compressione $r_1$ sarà calcolato come
\begin{equation}
r_1=\frac{s'}{s},
\label{tassopr}
\end{equation}
dove $s'$ rappresenta lo spazio occupato dalle connessioni della rete compressa utilizzando la rappresentazione matriciale CSC e $s$ lo spazio occupato occupato dalle connessioni della rete senza compressione.
Come ci accorgeremo nel Paragrafo~\ref{prpred} la compressione avviene effettivamente raggiungendo una certa sparsità\footnote{percentuale di elementi uguali a 0 nelle matrici}.

\section{Weight Sharing}
La tecnica del weight sharing viene utilizzata per ridurre lo spazio occupato per salvare le matrici dei pesi della rete neurale. Questa procedura consiste nel raggruppamento dei pesi simili, presi da una rete precedentemente addestrata, attraverso un algoritmo di clustering. Dopo per aver definito un centroide per ogni cluster, tutti i pesi vengono sostituiti nella rete con i centroidi più vicini.

\subsection{Strutture dati necessarie}
Per la gestione di questa procedura viene salvato un array che contine i valori dei centroidi e una matrice per salvare la corrispondenza peso-centroide (per ogni strato).
Denotiamo con $C$ il vettore dei centroidi e con $N$ la matrice delle corrispondenze, quindi
\begin{equation}
N_{ij} = \text{arg}\min\limits_{k}|C_{k}-w_{ij}|.
\label{ws}
\end{equation}
\subsection{Tecniche implementative}
Alla rete neurale base vengono aggiunte due procedure:
\begin{itemize}
\item una procedura che crea i vettori contenenti i $k$ centroidi, dove $k$ è il numero di cluster scelti (per ogni strato);
\item una procedura che crea la matrice $N$ definita in~\eqref{ws}.
\end{itemize}
Alla normale fase di training vengono aggiunte due procedure:
\begin{itemize}
\item una procedura che costruisce la matrice dei pesi effettiva per il feedforward con i valori dei centroidi invece dei valori originali
\begin{equation}
W'_{ij} = C_{N_{ij}};
\label{wprimo}
\end{equation}
\item una procedura per calcolare il gradiente dei centroidi tramite il \textit{cumulative gradient descent}~\cite{han2015compression}. Denotato con $\mathcal{L}$ il delta relativo alla funzione di errore, con $N$ la matrice degli indici dei cluster e con $C$ il vettore dei centroidi; il gradiente dei centroidi è calcolato come
\begin{equation}
\frac{\partial \mathcal{L}}{\partial c_{k}}=\sum\limits_{i,j}\frac{\partial \mathcal{L}}{\partial W_{ij}}1(N_{ij}=k).
\label{gradientews}
\end{equation}
\end{itemize}

\subsection{Tasso di compressione}
\label{sec:tassows}
Il tasso di compressione di questa tecnica dipende dal numero di cluster e dal numero di bit con cui vengono rappresentati
gli elementi delle matrici delle connessioni e i centroidi dei cluster. Denotiamo con $b$ il numero di bit con cui viene rappresentato un peso della rete e con $s$ il numero di connessioni nella rete, il tasso di compressione teorico $r_2 \in [0, 1]$ viene calcolato come
\begin{equation}
r_2=\frac{s\log_2 k+kb}{sb}.
\label{tassowsteo}
\end{equation}
$r_2$ rappresenta la percentuale di spazio occupato dalla rete compressa rispetto a quello originario.
Il linguaggio di programmazione utilizzato negli esperimenti permette di rappresentare numeri interi senza segno con 8 o 16 bit, per questo motivo il tasso di compressione effettivo diventa
\begin{equation}
r_2=\frac{s f(s) +32k}{32s}, \quad
f(s)=\begin{cases}
16 &\text{ se } s\geq255, \\
8 &\text{ altrimenti}.
\end{cases}
\label{tassows}
\end{equation}

\chapter{Esperimenti}
\label{esperimenti}
In questo capitolo spiegheremo gli esperimenti eseguiti su due problemi differenti:
\begin{itemize}
\item Cifre di MNIST: problema di classificazione, il dataset è composto da un insieme di immagini che rappresentano cifre scritte a mano,
\item Problema del predecessore: problema di regressione, i dataset sono composti da sequenze di numeri rappresentati in 64 bit.
\end{itemize}
\paragraph*{Classificazione e Regressione}
I classificatori separano i dati in due o più classi, nel caso di questo elaborato negli esperimenti con MNIST abbiamo 10 classi (le cifre tra 0 e 9).
I regressori invece si basano sull'interpolazione dei dati per associare tra loro due o più caratteristiche (\textit{feature}). Simile alla classificazione con la differenza che l'output ha un dominio continuo.
Entrambe le categorie sono affrontate, in questo elaborato, con apprendimento \textit{supervisionato}: si ha un insieme di input di esempio e il corrispettivo output desiderato con lo scopo di apprendere una regola generale in grado di mappare gli input negli output.

\section{MNIST}
Il dataset è una vasta base di dati di cifre scritte a mano, spesso impiegata nel campo dell'apprendimento automatico (\textit{machine learning}).
Il dataset contiene 60000 immagini di training e 10000 immagini di testing, nella Figura~\ref{fig:mnist} vengono mostrati alcuni esempi.
\begin{figure}
\begin{center}
\includegraphics[width=200pt]{MnistExamples.png}
\caption{Esempi MNIST~\cite{pict_mnist}}
\end{center}
\label{fig:mnist}
\end{figure}

\subsection{Addestramento e Tuning Parametri}
Per trovare una buona configurazione della rete abbiamo svolto una \textit{K-Fold Cross Validation}~\cite{Refaeilzadeh2009} con $k=3$.
\paragraph*{K-Fold Cross Validation}
Il \textit{training set} viene diviso in \textit{k} subset, a rotazione vengono usati come \textit{training set} $k-1$ subset, il rimanente subset viene usato come \textit{validation set} (un \textit{test set} fittizio).
In questa procedura il \textit{test set} non viene utilizzato perchè comprometterebbe la scelta del modello.
\begin{figure}
\begin{center}
\includegraphics[width=300pt]{K-fold_cross_validation_EN.jpg}
\caption{k-fold}\label{fig:kfold}
\end{center}
\end{figure}
Nella Figura~\ref{fig:kfold} possiamo vedere un esempio di splitting del \textit{training set} con $k=4$.\\
Negli esperimenti abbiamo fissato i seguenti parametri:
\begin{itemize}
\item $\eta=3\times 10^{-3}$;
\item funzione errore = errore quadratico medio;
\item metodo di apprendimento = discesa del gradiente con momentum ($\mu=0.99$);
\item funzione di attivazione degli strati \textit{hidden} = \textit{ReLU};
\item funzione di attivazione dello strato di output = \textit{softmax}\footnote{$\text{Softmax}(a_{i}) = \frac{\exp^{a_i}}{\sum_j \exp^{a_j}}$};
\item dimensione dei minibatch = 100 elementi;
\item criterio di arresto = 100 epoche;
\item inizializzazione pesi = $\matr{W^l}= \matr{randn}\sqrt{\frac{2}{size^{l-1}}}$  (He et al initialization~\cite{he2015delving}) dove $\matr{randn}$ è una matrice di numeri casuali estratti da una distribuzione normale standard, $l$ rappresenta lo strato e $size^{l-1}$ il numero di neuroni dello strato precedente.
\end{itemize}

I parametri di cui vogliamo trovare la configurazione migliore sono invece:
\begin{itemize}
\item architettura della rete (ovvero quanti strati hidden e quanti neuroni per ogni strato hidden);
\item dropout (percentuale di neuroni negli strati hidden non utilizzati nella fase di training).
\end{itemize}

\paragraph*{Dropout}
\begin{figure}
\begin{center}
\includegraphics[width=200pt]{dropout.jpeg}
\caption{dropout~\cite{10.5555/2627435.2670313}} \label{fig:dropout}
\end{center}
\end{figure}
Il \textit{dropout}~\cite{10.5555/2627435.2670313} è una tecnica utilizzata per evitare l'overfitting (ovvero imparare troppo dagli esempi del \textit{training set} senza imparare a generalizzare) mediante l'eliminazione di alcuni neuroni casuali in ogni strato ad eccezione dello strato di output. I neuroni da eliminare cambiano ad ogni epoca, durante la fase di predizione invece sono tutti attivi. Nella Figura~\ref{fig:dropout} possiamo vedere un esempio di questa tecnica.
Nell'elaborato è stata utilizzata la versione chiamata \textit{inverted dropout}
\begin{equation}
\matr{drop_l}=(\matr{rand} < p)/p,  \qquad
\matr{output_l}=\matr{output_l} * \matr{drop_l},
\label{drop}
\end{equation}
dove $\matr{output_l}$ rappresenta l'output di un generico strato $l$, mentre \textbf{rand} rappresenta una matrice di numeri casuali estratti da una distribuzione uniforme nell'intervallo $[0,1)$ della stessa dimensione di $\matr{output_l}$. $p$ invece è un parametro che rappresenta la probabilità di tenere attivo un neurone.

Nella Tabella~\ref{tab:modselmnist} e nella Tabella~\ref{tab:modselmnist2} la colonna accuratezza rappresenta la media delle accuratezze sui tre \textit{validation set}. Negli esperimenti effettuati abbiamo trovato come configurazione ottimale una rete MLP con uno strato hidden da 300 neuroni senza dropout (indicato nell'intestazione della tabella come $p=1$).
\input{modselmnist}
\subsection{Pruning}
\label{pruningmnist}
In questo paragrafo abbiamo applicato il pruning sulla rete scelta nella model selection, nella Tabella~\ref{tab:tabprmnist} sono riportati i risultati. 
Abbiamo applicato un pruning dal $10\%$ al $95\%$ a passi di $5\%$. Nella prima riga della Tabella~\ref{tab:tabprmnist} abbiamo riportato l'accuratezza sul \textit{test set} del modello scelto nel paragrafo precedente senza compressioni. L'accuratezza non varia di molto nei vari tagli, questi risultati sono dovuti al fatto che abbiamo usato un grande numero di neuroni nello strato hidden.
\vspace*{\fill}
\begin{center}
\begin{table}[]
  \small
  \caption{Pruning su MNIST}\label{tab:tabprmnist}
\begin{center}
\begin{tabular}{cc}

\hline\\[-11pt]

\hline\\[-11pt]
\texttt{\%Pruning} & \texttt{accuratezza} \\[1pt]
\hline\\[-6.5pt]
ALL & $98.34$ \\ 
$10\%$ & $98.34$ \\ 
$15\%$ & $98.34$ \\ 
$20\%$ & $98.34$ \\ 
$25\%$ & $98.35$ \\ 
$30\%$ & $98.36$ \\ 
$35\%$ & $98.35$ \\ 
$40\%$ & $98.36$ \\ 
$45\%$ & $98.36$ \\ 
$50\%$ & $98.36$ \\ 
$55\%$ & $98.32$ \\ 
$60\%$ & $98.3$ \\ 
$65\%$ & $98.33$ \\ 
$70\%$ & $98.34$ \\ 
$75\%$ & $98.34$ \\ 
$80\%$ & $98.38$ \\ 
$85\%$ & $98.31$ \\ 
$90\%$ & $98.27$ \\ 
$95\%$ & $98.28$ \\   
\hline\\[-11pt]
\hline\\[-8pt]
\end{tabular}\\[5pt]
\end{center}
\normalsize
\end{table}
\end{center}
\subsection{Weight Sharing}
In questo paragrafo abbiamo applicato il Weight Sharing sulla rete scelta nella model selection, nella Tabella~\ref{tab:tabwsmnist} sono riportati i risultati. La prima colonna rappresenta il numero di centroidi utilizzati per la compressione, prendendo per esempio $16$ - $8$: $16$ rappresenta il numero di centroidi nella matrice dei pesi tra lo strato di input e lo strato hidden; $8$ rappresenta il numero di centroidi nella matrice dei pesi tra lo strato hidden lo strato di output. Nella prima riga della Tabella~\ref{tab:tabwsmnist} abbiamo riportato l'accuratezza sul \textit{test set} del modello scelto senza compressioni. Come con il pruning possiamo notare che l'accuratezza, anche con un numero di centroidi abbastanza piccoli, (per esempio $16$ - $8$) rimane sopra il $98\%$.
\vspace*{\fill}
\begin{center}
\begin{table}[]
  \small
  \caption{Weight Sharing su MNIST}\label{tab:tabwsmnist}
\begin{center}
\begin{tabular}{cc}

\hline\\[-11pt]

\hline\\[-11pt]
\texttt{Centroidi} & \texttt{accuratezza} \\[1pt]
\hline\\[-6.5pt]
ALL & $98.34$ \\ 
$4$ - $2$ & $94.0$ \\ 
$16$ - $8$ & $98.14$ \\ 
$32$ - $8$ & $98.32$ \\ 
$32$ - $16$ & $98.25$ \\ 
$64$ - $16$ & $98.29$ \\ 
$64$ - $32$ & $98.34$ \\
$128$ - $32$ & $98.38$ \\ 
$192$ - $32$ & $98.38$ \\  
$192$ - $64$ & $98.36$ \\
$256$ - $32$ & $98.31$ \\ 
$256$ - $64$ & $98.36$ \\
$512$ - $32$ & $98.38$ \\  
$512$ - $64$ & $98.35$ \\
$1024$ - $32$ & $98.37$ \\  
$1024$ - $64$ & $98.37$ \\
$2048$ - $32$ & $98.36$ \\  
$2048$ - $64$ & $98.36$ \\ 
$4096$ - $32$ & $98.35$ \\ 
$4096$ - $64$ & $98.37$ \\
$8192$ - $32$ & $98.43$ \\
$8192$ - $64$ & $98.34$ \\  
\hline\\[-11pt]
\hline\\[-8pt]
\end{tabular}\\[5pt]
\end{center}
\normalsize
\end{table}
\end{center}

\section{Problema del predecessore}
Sia $X$ un sottoinsieme delle parti di un universo $U$, ordinato in base all'ordine $\leq$ definito su $U$. Partiamo dal presupposto che ogni elemento di $U$ può essere rappresentato da $d$ bit. Supponiamo anche che l'ordinamento lessicografico della rappresentazione binaria di ogni elemento in $U$ mantiene la relazione d'ordine $\leq$. Il problema di ricerca del predecessore consiste nel trovare la posizione della chiave più grande in $X$ non maggiore di una data in input $x$, indichiamo con $pos(x)$ la posizione di $x$ appartenente a $X$ nella sequenza ordinata, il problema è determinare $pred(x)=pos(z)$ con $z = \max y \in X$ dove $y \leq x$.
\begin{equation}
x=\{2,3,4,5,12,15,18\}
\tag{Esempio}
\end{equation}
Assumiamo che 7 è la chiave da cercare. La ricerca del predecessore dovrebbe restituire la posizione 4.
Dato $|X|=n$, sia $F_{X}$ la distribuzione cumulativa empirica degli elementi di $U$ rispetto a $X$, per ogni $x\in U$, $F_X(x)$ = ${\frac{|y \in X | y \leq z|}{n}}$. Per semplificare la notazione denotiamo $F_{X}$ con $F$. Nell'esempio sopra, $F(2)=\frac{1}{7}$, $F(5)=\frac{4}{7}$, $F(6)=\frac{4}{7}$, $F(18)=1$. La conoscenza di $F$ ci dà una soluzione al nostro problema, dato un elemento di $x$ appartenente a U, una sola valutazione di $F$ fornisce $pred(x)=\ceil{(F(z)*n)}$. Il nostro obiettivo è trovare una buona approssimazione $\tilde{F}$ di $F$.
La risoluzione di questo problema potrebbe consentire di trovare un elemento all'interno di una lista ordinata in tempo $O(1)$ invece che $O(\log n)$ degli alberi binari di ricerca.
La rete neurale però può compiere un errore nel dare la posizione di un elemento, per questo motivo teniamo traccia dell'errore massimo, indicato con $\epsilon$, che la rete compie. Questo perchè durante la ricerca, dopo che il modello ci ha dato la posizione, dobbiamo effettuare una ricerca di $\epsilon$ a destra e sinistra della posizione restituita dal modello. Per questo motivo negli esperimenti abbiamo provato a minimizzare una funzione convessa che approssima il massimo, ovvero la \textit{Log Sum Exp}, nel Paragrafo~\ref{seclse}.

Per questo problema sono stati utilizzati tre dataset, rispettivamente con $512$, $8192$ e $1048576$ esempi.
Diversamente da MNIST il modello MLP cercherà di risolvere un problema di regressione. La rete neurale dovrà imparare a restituire, data una chiave, $\tilde{F}$.
Sono state usate tre reti differenti:
\begin{itemize}
\item{rete 1 con 0 strati hidden};
\item{rete 2 con 1 strato hidden di 256 neuroni};
\item{rete 3 con 2 strati hidden di 256 neuroni}.
\end{itemize}
Per tutte e tre le reti, dove non indicato esplicitamente, i parametri utilizzati sono:
\begin{itemize}
\item funzione errore = errore quadratico medio;
\item metodo di apprendimento = discesa del gradiente con momentum ($\mu=0.9$);
\item funzione di attivazione dei neuroni = \textit{LeakyReLU} ($\alpha=0.05$);
\item{$\lambda$} = $10^{-5}$ \footnote{usato per nella regolarizzazione l2, valore aggiunto alla funzione di errore che penalizza i pesi più grandi, riducendo così il problema di overfitting ~\cite{L2}.};
\item{dimensione dei minibatch} = $64$;
\item{epoche} = $20000$;
\item{criterio di arresto} = l'apprendimento termina se non si hanno miglioramenti di performance con $\textit{patience}=4$ come spiegato nel Paragrafo~\ref{stopping};
\item inizializzazione dei pesi = numeri casuali estratti da una distribuzione normale standard con $\mu=0$ e $\sigma= 0.05$. 
\end{itemize}
Ci riferiremo a queste reti con \textbf{NNX}, dove X indica il numero di strati.
Per NN1 è stato usato $\eta=5 \times 10^{-4}$, per NN2 e NN3 invece $\eta=3 \times 10^{-3}$; questi valori sono stati trovati eseguendo un tuning su $\eta$.


Le colonne delle tabelle dei paragrafi successivi sono:
\begin{itemize}
\item pruning \% = percentuale di connessioni eliminate, calcolate con il quantile come viene spiegato nel Paragrafo~\ref{percentile}, 0 indica la rete non compressa;
\item $r_2$ = proporzione dello spazio originario occupato da quella compressa, 1 indica la rete non compressa;
\item cluster: indica, per ogni matrice dei pesi della rete, il numero di cluster utilizzati;
\item Space Overhead (KB) =  spazio del modello in KB rispetto al dataset;
\item training time = tempo in secondi impiegato dalla rete per il training, compreso il tempo iniziale di applicazione della compressione;
\item $\epsilon$ = errore massimo sugli esempi di training;
\item error \% = errore massimo rispetto alla dimensione del dataset considerato;
\item mean error = errore medio.
\end{itemize}

\subsection{Pruning}
\label{prpred}
In questo paragrafo abbiamo applicato il pruning e i risultati sono nelle Tabelle~\ref{pr13} -~\ref{pr310}. Come dicevamo in precedenza nel Paragrafo~\ref{sectassopr} prima di raggiungere una certa sparsità il modello compresso occupa più spazio del modello originario. I modelli compressi occupano meno spazio a partire da una percentuale di pruning compresa tra il 50\% e il 60\%.
Nei risultati di questi esperimenti possiamo notare una scarsa efficacia del pruning in reti con pochi neuroni (NN1), mentre (come già visto con MNIST nel Paragrafo~\ref{pruningmnist}) con molti neuroni (NN2 e NN3) il pruning a percentuali molto elevate mantiene errori medi ed errori massimi equivalenti alla rete non compressa se non migliori. 
\input{pruning}
\clearpage
\subsection{Weight Sharing}
In questo paragrafo abbiamo applicato il weight sharing e i risultati sono nelle Tabelle~\ref{ws13} -~\ref{ws310}.
Il numero di centroidi è calcolato come
\begin{equation}
\mathrm{cluster}=\left(\frac{r_2 \times32s -sb}{32}\right)
\label{cluster}
\end{equation}
dove $r_2$ è il tasso di compressione (Paragrafo~\ref{sec:tassows}), $s$ è il numero di connessioni e $b$ è il numero di byte utilizzati per rappresentare i centroidi.
Per~\eqref{cluster} e~\eqref{tassows} non possiamo raggiungere valori di $r_2$ inferiori a quelli mostrati nelle tabelle.
Come ci si aspetta, al crescere del numero di centroidi le reti hanno una performance crescente ed in alcuni casi migliore della rete non compressa. 
\input{ws}
\subsection{MSE vs MAE}
\label{secmsemae}
In questo paragrafo abbiamo provato a trovare una configurazione migliore per la rete NN1 confrontanto i risultati tra le funzioni errore Mean Squared Error (MSE) e Mean Absolute Error (MAE) descritte in~\eqref{msemae}.
\begin{equation}
\mathrm{MSE}=\frac{1}{n}\sum\limits^n_{i=1}{\left(t_i-y_i\right)}^2, \qquad
\mathrm{MAE}=\frac{1}{n}\sum\limits^n_{i=1}|t_i-y_i|.
\label{msemae}
\end{equation}
I risultati sono riportati nelle Tabelle~\ref{tab:mae1} -~\ref{tab:mae3}.
Dai risultati ottenuti si può vedere come tra le due diverse funzioni di errore non ci sia molta differenza di performance.
\input{mae}

\subsection{MSE vs LSE}
\label{seclse}
In questo paragrafo abbiamo provato a trovare una configurazione migliore per la rete NN1 confrontanto i risultati tra le funzioni errore Mean Squared Error (MSE, vista nel Paragrafo~\eqref{msemae}) e LogSumExp (LSE, descritta in~\eqref{LSE}).
I risultati sono riportati nelle Tabelle~\ref{tab:lse1} -~\ref{tab:lse3}.
Anche in questo confronto non abbiamo trovato molta differenza di performance.
\begin{equation}
\mathrm{LSE}(t_1-y_1,\dots ,t_n-y_n)=\log \left(\exp(t_1-y_1)+\cdots +\exp(t_n-y_n)\right)
\label{LSE}
\end{equation}
\input{lse}

\subsection{Splitting in N modelli}
L'idea applicata in questo paragrafo è quella di dividere il dataset ordinato in sottosequenze ordinate e usare una rete neurale senza strati hidden (\textbf{NN1}) per ogni sottosequenza con l'obiettivo di ridurre l'errore massimo $\epsilon$ che sarà quindi il massimo degli errori massimi di ogni modello.
I risultati sono riportati nelle Tabelle~\ref{tab:n1} -~\ref{tab:n3}, le intestazioni delle colonne sono:
\begin{itemize}
		\item Split = numero di split sul dataset (e di conseguenza numero di modelli NN1 usati);
		\item $\epsilon$ = massimo degli errori massimi di ogni NN1;
		\item $\mu$ = media degli errori massimi di ogni NN1;
		\item Space Overhead (KB) = somma dello spazio di ogni NN1 in KB rispetto al dataset.
	\end{itemize}
Come ci si aspetta, ad eccezione di alcuni casi, al crescere del numero di Split le performance migliorano occupando meno delle reti compresse precedenti. 
	\input{netofnets}
\bibliography{mybib}{}
\bibliographystyle{plain}
\end{document}
